{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chainer\n",
    "import chainer.functions as F\n",
    "import chainer.links as L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly import tools\n",
    "from plotly.graph_objs import *\n",
    "from plotly.offline import init_notebook_mode, iplot, iplot_mpl, plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test if running in Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_notebook():\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            return True   # Jupyter notebook or qtconsole\n",
    "        elif shell == 'TerminalInteractiveShell':\n",
    "            return False  # Terminal running IPython\n",
    "        else:\n",
    "            return False  # Other type (?)\n",
    "    except NameError:\n",
    "        return False      # Probably standard Python interpreter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action Constants\n",
    "act_dict = {0:\"HOLD\", 1:\"BUY\", 2:\"SELL\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment1:\n",
    "    \n",
    "    def __init__(self, data, tnx_cost=0, funding_cost=0, clip_rewards=False, debug=False, history_t=90):\n",
    "        self.data = data\n",
    "        self.history_t = history_t\n",
    "        self.tnx_cost = tnx_cost\n",
    "        self.funding_cost = funding_cost\n",
    "        self.clip_rewards = clip_rewards\n",
    "        self.debug = debug\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.t = 0\n",
    "        self.done = False\n",
    "        self.profits = 0\n",
    "        self.positions = []\n",
    "        self.position_value = 0\n",
    "        self.invested_capital = 0\n",
    "        self.history = [0 for _ in range(self.history_t)]\n",
    "        return [self.position_value] + self.history\n",
    "    \n",
    "    def step(self, act):\n",
    "        \n",
    "        if self.debug: print(f\"\\n\\nStep:{self.t} Action: {act_dict[act]}\")\n",
    "        \n",
    "        self.t += 1  \n",
    "        step_reward = 0\n",
    "        \n",
    "        prev_price = self.data.iloc[(self.t-1), :]['Close']\n",
    "        curr_price = self.data.iloc[self.t, :]['Close']\n",
    "        \n",
    "        # act = 0: stay, 1: buy, 2: sell\n",
    "        if act_dict[act] == 'BUY':\n",
    "            self.position_value = 0\n",
    "            \n",
    "            for trade_price in self.positions:\n",
    "                self.position_value += (curr_price - trade_price)\n",
    "            \n",
    "            self.positions.append(curr_price)\n",
    "            self.invested_capital += curr_price\n",
    "            if self.debug: print(f\"{act_dict[act]} @ {curr_price} New Position: {self.positions}\")\n",
    "\n",
    "            step_reward = curr_price * self.tnx_cost  # adding tnx cost to discourage unecessary frequent buying  \n",
    "            \n",
    "        elif act_dict[act] == 'SELL': # sell\n",
    "            self.position_value = 0\n",
    "\n",
    "            if len(self.positions) == 0:\n",
    "                step_reward = 0   # if no position then no reward, tnx cost nor funding cost\n",
    "                if self.debug: print(f\"Action:{act_dict[act]} No open position. Reward:{step_reward}\")\n",
    "            \n",
    "            else:\n",
    "                step_profits = 0\n",
    "                \n",
    "                for trade_price in self.positions:\n",
    "                    trade_profit = (curr_price - trade_price)\n",
    "                    step_profits += trade_profit\n",
    "                    if self.debug: print(f\"{act_dict[act]} @ {curr_price} for purchase @ {trade_price}. Profit: {trade_profit}. Cumm step profit: {step_profits}\")\n",
    "                \n",
    "                step_reward = step_profits\n",
    "                self.profits += step_profits\n",
    "                if self.debug: print(f\"Step Profit = Step Reward:{step_reward} Cummulative Strategy Profits:{self.profits}\")\n",
    "                \n",
    "                self.positions = []\n",
    "                self.invested_capital = 0\n",
    "                \n",
    "        elif act_dict[act] == 'HOLD':\n",
    "            self.position_value = 0\n",
    "            \n",
    "            for trade_price in self.positions:\n",
    "                self.position_value += (curr_price - trade_price)\n",
    "            \n",
    "            step_reward = self.invested_capital * self.funding_cost ### only if there is an open position value\n",
    "            if self.debug: print(f\"{act_dict[act]} Curr Position:{self.positions} InvestedCap:{self.invested_capital} MTM:{self.position_value} Reward:{step_reward}\")\n",
    "            \n",
    "        else: \n",
    "            print(f\"Invalid Action:{act}:{act_dict[act]}\")\n",
    "        \n",
    "               \n",
    "        # Generate the returns history to be used by DQN for training & prediction.\n",
    "        self.history.pop(0)\n",
    "        self.history.append(curr_price - prev_price)\n",
    "               \n",
    "        reward = step_reward\n",
    "        if self.clip_rewards: \n",
    "            if   step_reward > 0: reward = 1\n",
    "            elif step_reward < 0: reward = -1 \n",
    "        \n",
    "        if self.debug: print(f\"Raw Reward:{step_reward}  Actual Reward:{reward}\")  # this is the reward at current step \n",
    "        \n",
    "        return [self.position_value] + self.history , reward, self.done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_Network(chainer.Chain):\n",
    "\n",
    "    def __init__(self, input_size=91, hidden_size=100, output_size=3):\n",
    "        super(Q_Network, self).__init__(\n",
    "            fc1 = L.Linear(input_size, hidden_size),\n",
    "            fc2 = L.Linear(hidden_size, hidden_size),\n",
    "            fc3 = L.Linear(hidden_size, output_size)\n",
    "        )\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        y = self.fc3(h)\n",
    "        return y\n",
    "\n",
    "    def reset(self):\n",
    "        self.zerograds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Candlestick plot of train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_test(train=[], test=[], date_split=''):\n",
    "    \n",
    "    data = []\n",
    "    if len(train) > 0:\n",
    "        data += [Candlestick(x=train.index, open=train['Open'], high=train['High'], low=train['Low'], close=train['Close'], \\\n",
    "                    name='train')]\n",
    "    if len(test) > 0:\n",
    "        data += [Candlestick(x=test.index, open=test['Open'], high=test['High'], low=test['Low'], close=test['Close'], \\\n",
    "                    name='test')]\n",
    "    \n",
    "    if date_split != '':\n",
    "        layout = {\n",
    "             'shapes': [\n",
    "                 {'x0': date_split, 'x1': date_split, 'y0': 0, 'y1': 1, 'xref': 'x', 'yref': 'paper', \\\n",
    "                  'line': {'color': 'rgb(0,0,0)', 'width': 1}}\n",
    "             ],\n",
    "            'annotations': [\n",
    "                {'x': date_split, 'y': 1.0, 'xref': 'x', 'yref': 'paper', 'showarrow': False, 'xanchor': 'left', \\\n",
    "                 'text': ' test data'},\n",
    "                {'x': date_split, 'y': 1.0, 'xref': 'x', 'yref': 'paper', 'showarrow': False, 'xanchor': 'right', \\\n",
    "                 'text': 'train data '}\n",
    "            ]\n",
    "        }\n",
    "        figure = Figure(data=data, layout=layout)\n",
    "    else:\n",
    "        figure = Figure(data=data)\n",
    "    \n",
    "    if is_notebook():\n",
    "        iplot(figure)\n",
    "    else:\n",
    "        plot(figure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying of Strategy on Train & Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_trained_model(Policy_Network, train_env=Environment1([]), test_env=Environment1([])):\n",
    "    \n",
    "    # train\n",
    "    train_actions = []\n",
    "    train_rewards = []\n",
    "    train_invested_capital = []\n",
    "    train_position_value = []\n",
    "    train_profits = []\n",
    "    if len(train_env.data) != 0:\n",
    "        state = train_env.reset()\n",
    "\n",
    "        for _ in range(len(train_env.data)-1):\n",
    "\n",
    "            action_value = Policy_Network(np.array(state, dtype=np.float32).reshape(1, -1))\n",
    "\n",
    "            # if there are no positions to sell and highest action value is \"Sell\", choose second best action\n",
    "            if len(train_env.positions) == 0 and np.argmax(action_value.data) == 2:\n",
    "                action_value.data[0][np.argmax(action_value.data)] = np.min(action_value.data)\n",
    "\n",
    "            action = np.argmax(action_value.data)\n",
    "            next_state, reward, done = train_env.step(action)\n",
    "\n",
    "            train_actions.append(action)\n",
    "            train_rewards.append(reward)\n",
    "            train_invested_capital.append(train_env.invested_capital)\n",
    "            train_position_value.append(train_env.position_value)\n",
    "            train_profits.append(train_env.profits)\n",
    "\n",
    "            state = next_state\n",
    "    \n",
    "    # test\n",
    "    test_actions = []\n",
    "    test_rewards = []\n",
    "    test_invested_capital = []\n",
    "    test_position_value = []\n",
    "    test_profits = []\n",
    "    if len(test_env.data) != 0:\n",
    "        state = test_env.reset()\n",
    "\n",
    "        for _ in range(len(test_env.data)-1):\n",
    "\n",
    "            action_value = Policy_Network(np.array(state, dtype=np.float32).reshape(1, -1))\n",
    "\n",
    "            # if there are no positions to sell and highest action value is \"Sell\", choose second best action\n",
    "            if len(test_env.positions) == 0 and np.argmax(action_value.data) == 2:\n",
    "                action_value.data[0][np.argmax(action_value.data)] = np.min(action_value.data)\n",
    "\n",
    "            action = np.argmax(action_value.data)\n",
    "            next_state, reward, done = test_env.step(action)\n",
    "\n",
    "            test_actions.append(action)\n",
    "            test_rewards.append(reward)\n",
    "            test_invested_capital.append(test_env.invested_capital)\n",
    "            test_position_value.append(test_env.position_value)\n",
    "            test_profits.append(test_env.profits)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "    return train_env, test_env, \\\n",
    "           train_actions, train_rewards, train_invested_capital, train_position_value, train_profits, \\\n",
    "           test_actions, test_rewards, test_invested_capital, test_position_value, test_profits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot of the actions on candlestick price movements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_test_by_q(algorithm_name, \n",
    "                         train_env=Environment1([]), \n",
    "                         test_env=Environment1([]), \n",
    "                         date_split='',\n",
    "                         train_actions=[],\n",
    "                         train_rewards=[],\n",
    "                         train_position_value=[],\n",
    "                         train_profits=[],\n",
    "                         test_actions=[],\n",
    "                         test_rewards=[],\n",
    "                         test_position_value=[],\n",
    "                         test_profits=[]):\n",
    "\n",
    "    data = []\n",
    "    act_color0, act_color1, act_color2 = 'gray', 'cyan', 'magenta'\n",
    "    \n",
    "    if len(train_env.data) != 0:\n",
    "        train_copy = train_env.data.copy()\n",
    "        train_copy['action'] = train_actions + [np.nan]\n",
    "        train_copy['reward'] = train_rewards + [np.nan]\n",
    "        train0 = train_copy[train_copy['action'] == 0]\n",
    "        train1 = train_copy[train_copy['action'] == 1]\n",
    "        train2 = train_copy[train_copy['action'] == 2]\n",
    "\n",
    "        data += [\n",
    "            Candlestick(x=train0.index, open=train0['Open'], high=train0['High'], low=train0['Low'], close=train0['Close'], \\\n",
    "                        increasing=dict(line=dict(color=act_color0)), decreasing=dict(line=dict(color=act_color0))),\n",
    "            Candlestick(x=train1.index, open=train1['Open'], high=train1['High'], low=train1['Low'], close=train1['Close'], \\\n",
    "                        increasing=dict(line=dict(color=act_color1)), decreasing=dict(line=dict(color=act_color1))),\n",
    "            Candlestick(x=train2.index, open=train2['Open'], high=train2['High'], low=train2['Low'], close=train2['Close'], \\\n",
    "                        increasing=dict(line=dict(color=act_color2)), decreasing=dict(line=dict(color=act_color2)))\n",
    "        ]\n",
    "    \n",
    "    if len(test_env.data) != 0:\n",
    "        test_copy = test_env.data.copy()\n",
    "        test_copy['action'] = test_actions + [np.nan]\n",
    "        test_copy['reward'] = test_rewards + [np.nan]\n",
    "        test0 = test_copy[test_copy['action'] == 0]\n",
    "        test1 = test_copy[test_copy['action'] == 1]\n",
    "        test2 = test_copy[test_copy['action'] == 2]\n",
    "\n",
    "        data += [\n",
    "            Candlestick(x=test0.index, open=test0['Open'], high=test0['High'], low=test0['Low'], close=test0['Close'], \\\n",
    "                        increasing=dict(line=dict(color=act_color0)), decreasing=dict(line=dict(color=act_color0))),\n",
    "            Candlestick(x=test1.index, open=test1['Open'], high=test1['High'], low=test1['Low'], close=test1['Close'], \\\n",
    "                        increasing=dict(line=dict(color=act_color1)), decreasing=dict(line=dict(color=act_color1))),\n",
    "            Candlestick(x=test2.index, open=test2['Open'], high=test2['High'], low=test2['Low'], close=test2['Close'], \\\n",
    "                        increasing=dict(line=dict(color=act_color2)), decreasing=dict(line=dict(color=act_color2)))\n",
    "        ]\n",
    "    \n",
    "    title = '{}:'.format(algorithm_name)\n",
    "    if len(train_env.data) != 0:\n",
    "        title += 'train s-reward {}, profits {} '.format(int(sum(train_rewards)), int(train_profits[-1]))\n",
    "    if len(test_env.data) != 0:\n",
    "        title += 'test s-reward {}, profits {}'.format(int(sum(test_rewards)), int(test_profits[-1]))\n",
    "\n",
    "    layout = {\n",
    "        'title': title,\n",
    "        'showlegend': False\n",
    "    }\n",
    "                \n",
    "    if date_split != '':\n",
    "        layout['shapes'] = [\n",
    "                 {'x0': date_split, 'x1': date_split, 'y0': 0, 'y1': 1, 'xref': 'x', 'yref': 'paper', 'line': {'color': 'rgb(0,0,0)', 'width': 1}}\n",
    "             ]\n",
    "        layout['annotations'] = [\n",
    "                {'x': date_split, 'y': 1.0, 'xref': 'x', 'yref': 'paper', 'showarrow': False, 'xanchor': 'left', 'text': ' test data'},\n",
    "                {'x': date_split, 'y': 1.0, 'xref': 'x', 'yref': 'paper', 'showarrow': False, 'xanchor': 'right', 'text': 'train data '}\n",
    "            ]\n",
    "        \n",
    "    figure = Figure(data=data, layout=layout)\n",
    "    \n",
    "    if is_notebook():\n",
    "        iplot(figure)\n",
    "    else:\n",
    "        plot(figure)\n",
    "        \n",
    "    print(\"Gray: HOLD, Cyan: BUY, Magenta: SELL\")\n",
    "    print(\"Max Position Value: \" + str(max(train_position_value+test_position_value)))\n",
    "    print(\"Min Position Value: \" + str(min(train_position_value+test_position_value)))\n",
    "    print(\"Last Action Taken: \" + str(act_dict[(train_actions+test_actions)[-1]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
