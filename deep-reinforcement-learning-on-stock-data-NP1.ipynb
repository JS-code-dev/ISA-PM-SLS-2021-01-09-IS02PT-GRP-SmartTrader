{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "774f1f3d-5353-4a9f-858c-0cbd993577f8",
    "_uuid": "0d30485734f2e1df31e62309ed20446b987dfea8"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "936eed8cc53b525db200513139019e94e0a89757"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chainer\n",
    "import chainer.functions as F\n",
    "import chainer.links as L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly import tools\n",
    "from plotly.graph_objs import *\n",
    "from plotly.offline import init_notebook_mode, iplot, iplot_mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nirav\\WorkArea\\DeepRL_on_StockData\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('c:\\\\Users\\\\nirav\\\\WorkArea\\\\DeepRL_on_StockData')\n",
    "print (os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4d5e73df952e3865515ba33fffabfea59b52c7e9"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('./Data/Stocks/goog.us.txt')\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data = data.set_index('Date')\n",
    "print(data.index.min(), data.index.max())\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "10fddbff0b397113861a9a3c628c98773d3bbdd6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(446, 470)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_split = '2016-01-01'\n",
    "train = data[:date_split]\n",
    "test = data[date_split:]\n",
    "len(train), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "e9d01773ac00cb8e280809042097ba1502d519e8"
   },
   "outputs": [],
   "source": [
    "def plot_train_test(train, test, date_split):\n",
    "    \n",
    "    data = [\n",
    "        Candlestick(x=train.index, open=train['Open'], high=train['High'], low=train['Low'], close=train['Close'], \\\n",
    "                    name='train'),\n",
    "        Candlestick(x=test.index, open=test['Open'], high=test['High'], low=test['Low'], close=test['Close'], \\\n",
    "                    name='test')\n",
    "    ]\n",
    "    layout = {\n",
    "         'shapes': [\n",
    "             {'x0': date_split, 'x1': date_split, 'y0': 0, 'y1': 1, 'xref': 'x', 'yref': 'paper', \\\n",
    "              'line': {'color': 'rgb(0,0,0)', 'width': 1}}\n",
    "         ],\n",
    "        'annotations': [\n",
    "            {'x': date_split, 'y': 1.0, 'xref': 'x', 'yref': 'paper', 'showarrow': False, 'xanchor': 'left', \\\n",
    "             'text': ' test data'},\n",
    "            {'x': date_split, 'y': 1.0, 'xref': 'x', 'yref': 'paper', 'showarrow': False, 'xanchor': 'right', \\\n",
    "             'text': 'train data '}\n",
    "        ]\n",
    "    }\n",
    "    figure = Figure(data=data, layout=layout)\n",
    "    iplot(figure)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "000f1c1c78598a1d39c56357fa04dbe94379bb35"
   },
   "outputs": [],
   "source": [
    "plot_train_test(train, test, date_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "1fc5d9a253b7cef891467c54bd61cbca3c623b80"
   },
   "outputs": [],
   "source": [
    "class Environment1:\n",
    "    \n",
    "    def __init__(self, data, history_t=90): # history_t used to ??\n",
    "        self.data = data\n",
    "        self.history_t = history_t\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.t = 0\n",
    "        self.done = False    # used for ??\n",
    "        self.profits = 0\n",
    "        self.positions = []\n",
    "        self.position_value = 0\n",
    "        self.history = [0 for _ in range(self.history_t)]\n",
    "        return [self.position_value] + self.history # obs\n",
    "    \n",
    "    def step(self, act):\n",
    "        reward = 0\n",
    "        \n",
    "        # act = 0: stay, 1: buy, 2: sell\n",
    "        if act == 1:  # buy\n",
    "            self.positions.append(self.data.iloc[self.t, :]['Close']) # add closing price to position\n",
    "            \n",
    "        elif act == 2: # sell\n",
    "            if len(self.positions) == 0: reward = -1   # if no position then negative rewards\n",
    "                \n",
    "            else:\n",
    "                profits = 0\n",
    "                for p in self.positions:\n",
    "                    profits += (self.data.iloc[self.t, :]['Close'] - p) # calculate cumulative profit for each 'buy'\n",
    "                reward += profits  # add this profit to reward (rewards should be the same as profits in every step??)\n",
    "                self.profits += profits  # add step profits to overall portfolio profits\n",
    "                self.positions = [] # flatten portfolio position to zero\n",
    "        \n",
    "        # set next time\n",
    "        self.t += 1   # increment step counter\n",
    "        self.position_value = 0  # initialize\n",
    "        for p in self.positions:\n",
    "            self.position_value += (self.data.iloc[self.t, :]['Close'] - p) # MTM the postfolio positions. Only for a BUY/HOLD??? As SELL step has flatenned portfolio\n",
    "        self.history.pop(0)\n",
    "        self.history.append(self.data.iloc[self.t, :]['Close'] - self.data.iloc[(self.t-1), :]['Close'])\n",
    "        \n",
    "        # clipping reward\n",
    "        if   reward > 0: reward = 1\n",
    "        elif reward < 0: reward = -1\n",
    "        \n",
    "        return [self.position_value] + self.history , reward, self.done # obs, reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "0fa94631ffc3408de7c1af0802283c32cda036a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.5299999999999727], 0, False)\n",
      "([-3.019999999999982, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.5299999999999727, -3.019999999999982], 0, False)\n",
      "([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.5299999999999727, -3.019999999999982, 10.18999999999994], -1, False)\n",
      "([-0.15999999999996817, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.5299999999999727, -3.019999999999982, 10.18999999999994, -0.15999999999996817], 0, False)\n",
      "([5.32000000000005, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.5299999999999727, -3.019999999999982, 10.18999999999994, -0.15999999999996817, 2.740000000000009], 0, False)\n"
     ]
    }
   ],
   "source": [
    "env = Environment1(train)\n",
    "print(env.reset())\n",
    "for _ in range(5):  # how many steps\n",
    "    pact = np.random.randint(3) # select action at random\n",
    "    print(env.step(pact))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "7e34a19d54e2888d5cb618e5b7e02b0a0fc633bd"
   },
   "outputs": [],
   "source": [
    "# DQN\n",
    "\n",
    "def train_dqn(env):\n",
    "\n",
    "    class Q_Network(chainer.Chain):\n",
    "\n",
    "        def __init__(self, input_size, hidden_size, output_size):\n",
    "            super(Q_Network, self).__init__(\n",
    "                fc1 = L.Linear(input_size, hidden_size),\n",
    "                fc2 = L.Linear(hidden_size, hidden_size),\n",
    "                fc3 = L.Linear(hidden_size, output_size)\n",
    "            )\n",
    "\n",
    "        def __call__(self, x):\n",
    "            h = F.relu(self.fc1(x))\n",
    "            h = F.relu(self.fc2(h))\n",
    "            y = self.fc3(h)\n",
    "            return y\n",
    "\n",
    "        def reset(self):\n",
    "            self.zerograds()\n",
    "\n",
    "    Q = Q_Network(input_size=env.history_t+1, hidden_size=100, output_size=3)\n",
    "    Q_ast = copy.deepcopy(Q)\n",
    "    optimizer = chainer.optimizers.Adam()\n",
    "    optimizer.setup(Q)\n",
    "\n",
    "    epoch_num = 50\n",
    "    step_max = len(env.data)-1\n",
    "    memory_size = 200\n",
    "    batch_size = 20\n",
    "    epsilon = 1.0\n",
    "    epsilon_decrease = 1e-3\n",
    "    epsilon_min = 0.1\n",
    "    start_reduce_epsilon = 200\n",
    "    train_freq = 10\n",
    "    update_q_freq = 20\n",
    "    gamma = 0.97\n",
    "    show_log_freq = 5\n",
    "\n",
    "    memory = []\n",
    "    total_step = 0\n",
    "    total_rewards = []\n",
    "    total_losses = []\n",
    "\n",
    "    start = time.time()\n",
    "    for epoch in range(epoch_num):\n",
    "\n",
    "        pobs = env.reset()\n",
    "        step = 0\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        total_loss = 0\n",
    "\n",
    "        while not done and step < step_max:\n",
    "\n",
    "            # select act\n",
    "            pact = np.random.randint(3)\n",
    "            if np.random.rand() > epsilon:\n",
    "                pact = Q(np.array(pobs, dtype=np.float32).reshape(1, -1))\n",
    "                pact = np.argmax(pact.data)\n",
    "\n",
    "            # act\n",
    "            obs, reward, done = env.step(pact)\n",
    "\n",
    "            # add memory\n",
    "            memory.append((pobs, pact, reward, obs, done))\n",
    "            if len(memory) > memory_size:\n",
    "                memory.pop(0)\n",
    "\n",
    "            # train or update q\n",
    "            if len(memory) == memory_size:\n",
    "                if total_step % train_freq == 0:\n",
    "                    shuffled_memory = np.random.permutation(memory)\n",
    "                    memory_idx = range(len(shuffled_memory))\n",
    "                    for i in memory_idx[::batch_size]:\n",
    "                        batch = np.array(shuffled_memory[i:i+batch_size])\n",
    "                        b_pobs = np.array(batch[:, 0].tolist(), dtype=np.float32).reshape(batch_size, -1)\n",
    "                        b_pact = np.array(batch[:, 1].tolist(), dtype=np.int32)\n",
    "                        b_reward = np.array(batch[:, 2].tolist(), dtype=np.int32)\n",
    "                        b_obs = np.array(batch[:, 3].tolist(), dtype=np.float32).reshape(batch_size, -1)\n",
    "                        b_done = np.array(batch[:, 4].tolist(), dtype=np.bool)\n",
    "\n",
    "                        q = Q(b_pobs)\n",
    "                        maxq = np.max(Q_ast(b_obs).data, axis=1)\n",
    "                        target = copy.deepcopy(q.data)\n",
    "                        for j in range(batch_size):\n",
    "                            target[j, b_pact[j]] = b_reward[j]+gamma*maxq[j]*(not b_done[j])\n",
    "                        Q.reset()\n",
    "                        loss = F.mean_squared_error(q, target)\n",
    "                        total_loss += loss.data\n",
    "                        loss.backward()\n",
    "                        optimizer.update()\n",
    "\n",
    "                if total_step % update_q_freq == 0:\n",
    "                    Q_ast = copy.deepcopy(Q)\n",
    "\n",
    "            # epsilon\n",
    "            if epsilon > epsilon_min and total_step > start_reduce_epsilon:\n",
    "                epsilon -= epsilon_decrease\n",
    "\n",
    "            # next step\n",
    "            total_reward += reward\n",
    "            pobs = obs\n",
    "            step += 1\n",
    "            total_step += 1\n",
    "\n",
    "        total_rewards.append(total_reward)\n",
    "        total_losses.append(total_loss)\n",
    "\n",
    "        if (epoch+1) % show_log_freq == 0:\n",
    "            log_reward = sum(total_rewards[((epoch+1)-show_log_freq):])/show_log_freq\n",
    "            log_loss = sum(total_losses[((epoch+1)-show_log_freq):])/show_log_freq\n",
    "            elapsed_time = time.time()-start\n",
    "            print('\\t'.join(map(str, [epoch+1, epsilon, total_step, log_reward, log_loss, elapsed_time])))\n",
    "            start = time.time()\n",
    "            \n",
    "    return Q, total_losses, total_rewards\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "09836d3d2f07a0edd74224a6bcd01192cf84caa6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nirav\\anaconda3\\envs\\sls\\lib\\site-packages\\numpy\\core\\_asarray.py:83: VisibleDeprecationWarning:\n",
      "\n",
      "Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\t0.0999999999999992\t2225\t-33.8\t2210.9205656416716\t12.19037914276123\n",
      "10\t0.0999999999999992\t4450\t-29.8\t270.8124801587313\t14.482247591018677\n",
      "15\t0.0999999999999992\t6675\t-14.6\t40.26095383064821\t11.914119958877563\n",
      "20\t0.0999999999999992\t8900\t3.6\t45.045509819220754\t13.914765357971191\n",
      "25\t0.0999999999999992\t11125\t1.4\t17.97384876490105\t16.151780128479004\n",
      "30\t0.0999999999999992\t13350\t8.2\t8.44977583156433\t13.247552156448364\n",
      "35\t0.0999999999999992\t15575\t7.8\t12.962938816426322\t18.02975559234619\n",
      "40\t0.0999999999999992\t17800\t9.6\t9.454462162719574\t15.090619564056396\n",
      "45\t0.0999999999999992\t20025\t13.4\t8.162279916414992\t18.177358865737915\n",
      "50\t0.0999999999999992\t22250\t11.0\t7.14566634632647\t13.919752836227417\n"
     ]
    }
   ],
   "source": [
    "Q, total_losses, total_rewards = train_dqn(Environment1(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "73e0c0791daf22632f473ef371d2d62719331a2b"
   },
   "outputs": [],
   "source": [
    "def plot_loss_reward(total_losses, total_rewards):\n",
    "\n",
    "    figure = tools.make_subplots(rows=1, cols=2, subplot_titles=('loss', 'reward'), print_grid=False)\n",
    "    figure.append_trace(Scatter(y=total_losses, mode='lines', line=dict(color='skyblue')), 1, 1)\n",
    "    figure.append_trace(Scatter(y=total_rewards, mode='lines', line=dict(color='orange')), 1, 2)\n",
    "    figure['layout']['xaxis1'].update(title='epoch')\n",
    "    figure['layout']['xaxis2'].update(title='epoch')\n",
    "    figure['layout'].update(height=400, width=900, showlegend=False)\n",
    "    iplot(figure)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "13cc8986e9d721d143c2a5a488865e2a27c882ec"
   },
   "outputs": [],
   "source": [
    "plot_loss_reward(total_losses, total_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "45856d723a14d02777475ac772593dc55319a9fe"
   },
   "outputs": [],
   "source": [
    "def plot_train_test_by_q(train_env, test_env, Q, algorithm_name):\n",
    "    \n",
    "    # train\n",
    "    pobs = train_env.reset()\n",
    "    train_acts = []\n",
    "    train_rewards = []\n",
    "\n",
    "    for _ in range(len(train_env.data)-1):\n",
    "        \n",
    "        pact = Q(np.array(pobs, dtype=np.float32).reshape(1, -1))\n",
    "        pact = np.argmax(pact.data)\n",
    "        train_acts.append(pact)\n",
    "            \n",
    "        obs, reward, done = train_env.step(pact)\n",
    "        train_rewards.append(reward)\n",
    "\n",
    "        pobs = obs\n",
    "        \n",
    "    train_profits = train_env.profits\n",
    "    \n",
    "    # test\n",
    "    pobs = test_env.reset()\n",
    "    test_acts = []\n",
    "    test_rewards = []\n",
    "\n",
    "    for _ in range(len(test_env.data)-1):\n",
    "    \n",
    "        pact = Q(np.array(pobs, dtype=np.float32).reshape(1, -1))\n",
    "        pact = np.argmax(pact.data)\n",
    "        test_acts.append(pact)\n",
    "            \n",
    "        obs, reward, done = test_env.step(pact)\n",
    "        test_rewards.append(reward)\n",
    "\n",
    "        pobs = obs\n",
    "        \n",
    "    test_profits = test_env.profits\n",
    "    \n",
    "    # plot\n",
    "    train_copy = train_env.data.copy()\n",
    "    test_copy = test_env.data.copy()\n",
    "    train_copy['act'] = train_acts + [np.nan]\n",
    "    train_copy['reward'] = train_rewards + [np.nan]\n",
    "    test_copy['act'] = test_acts + [np.nan]\n",
    "    test_copy['reward'] = test_rewards + [np.nan]\n",
    "    train0 = train_copy[train_copy['act'] == 0]\n",
    "    train1 = train_copy[train_copy['act'] == 1]\n",
    "    train2 = train_copy[train_copy['act'] == 2]\n",
    "    test0 = test_copy[test_copy['act'] == 0]\n",
    "    test1 = test_copy[test_copy['act'] == 1]\n",
    "    test2 = test_copy[test_copy['act'] == 2]\n",
    "    act_color0, act_color1, act_color2 = 'gray', 'cyan', 'magenta'\n",
    "\n",
    "    data = [\n",
    "        Candlestick(x=train0.index, open=train0['Open'], high=train0['High'], low=train0['Low'], close=train0['Close'], \\\n",
    "                    increasing=dict(line=dict(color=act_color0)), decreasing=dict(line=dict(color=act_color0))),\n",
    "        Candlestick(x=train1.index, open=train1['Open'], high=train1['High'], low=train1['Low'], close=train1['Close'], \\\n",
    "                    increasing=dict(line=dict(color=act_color1)), decreasing=dict(line=dict(color=act_color1))),\n",
    "        Candlestick(x=train2.index, open=train2['Open'], high=train2['High'], low=train2['Low'], close=train2['Close'], \\\n",
    "                    increasing=dict(line=dict(color=act_color2)), decreasing=dict(line=dict(color=act_color2))),\n",
    "        \n",
    "        Candlestick(x=test0.index, open=test0['Open'], high=test0['High'], low=test0['Low'], close=test0['Close'], \\\n",
    "                    increasing=dict(line=dict(color=act_color0)), decreasing=dict(line=dict(color=act_color0))),\n",
    "        Candlestick(x=test1.index, open=test1['Open'], high=test1['High'], low=test1['Low'], close=test1['Close'], \\\n",
    "                    increasing=dict(line=dict(color=act_color1)), decreasing=dict(line=dict(color=act_color1))),\n",
    "        Candlestick(x=test2.index, open=test2['Open'], high=test2['High'], low=test2['Low'], close=test2['Close'], \\\n",
    "                    increasing=dict(line=dict(color=act_color2)), decreasing=dict(line=dict(color=act_color2)))\n",
    "    ]\n",
    "    title = '{}: train s-reward {}, profits {}, test s-reward {}, profits {}'.format(\n",
    "        algorithm_name,\n",
    "        int(sum(train_rewards)),\n",
    "        int(train_profits),\n",
    "        int(sum(test_rewards)),\n",
    "        int(test_profits)\n",
    "    )\n",
    "    layout = {\n",
    "        'title': title,\n",
    "        'showlegend': False,\n",
    "         'shapes': [\n",
    "             {'x0': date_split, 'x1': date_split, 'y0': 0, 'y1': 1, 'xref': 'x', 'yref': 'paper', 'line': {'color': 'rgb(0,0,0)', 'width': 1}}\n",
    "         ],\n",
    "        'annotations': [\n",
    "            {'x': date_split, 'y': 1.0, 'xref': 'x', 'yref': 'paper', 'showarrow': False, 'xanchor': 'left', 'text': ' test data'},\n",
    "            {'x': date_split, 'y': 1.0, 'xref': 'x', 'yref': 'paper', 'showarrow': False, 'xanchor': 'right', 'text': 'train data '}\n",
    "        ]\n",
    "    }\n",
    "    figure = Figure(data=data, layout=layout)\n",
    "    iplot(figure)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "00243e4457ad8c355a466b1672fb5532d2ee3793"
   },
   "outputs": [],
   "source": [
    "plot_train_test_by_q(Environment1(train), Environment1(test), Q, 'DQN')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
